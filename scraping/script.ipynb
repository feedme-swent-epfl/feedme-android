{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Global sets to store unique ingredients and categories\n",
    "all_ingredients = set()\n",
    "all_categories = set()\n",
    "\n",
    "## Extracts all recipe URLs from a given page. ##\n",
    "def get_recipes_from_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    recipe_links = [a['href'] for a in soup.find_all('a', href=True) if \"/recipes/\" in a['href']]\n",
    "    return recipe_links\n",
    "\n",
    "\n",
    "## Finds the URL for the next page based on alphabetical navigation. ##\n",
    "def generate_urls(base_url):\n",
    "    parts = base_url.split(\"123\")\n",
    "    urls = [parts[0] + letter + parts[1] for letter in list(map(chr, range(97, 120)))]  # 'a' to 'w'\n",
    "    urls.append(parts[0] + \"xyz\" + parts[1])  # Adding 'xyz'\n",
    "    return urls\n",
    "\n",
    "\n",
    "## Scrapes detailed information about a recipe from its page. ##\n",
    "def scrape_recipe_details(url):\n",
    "    time.sleep(random.uniform(5.0, 10.0))  # Random delay between requests\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    response = requests.get(url)\n",
    "            \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract data using BeautifulSoup's methods\n",
    "        title = soup.find('span', class_=\"o-AssetTitle__a-HeadlineText\").get_text(strip=True)\n",
    "        level_headline = soup.find('span', class_='o-RecipeInfo__a-Headline')\n",
    "        level = level_headline.find_next_sibling('span').get_text(strip=True) if level_headline else 'Level not found'\n",
    "        yield_headline = soup.find('span', string='Yield:')\n",
    "        servings = yield_headline.find_next_sibling('span').get_text(strip=True) if yield_headline else 'Servings not found'\n",
    "        cooking_time = soup.find('span', class_=\"o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total\").get_text(strip=True)\n",
    "        ingredients_list = soup.find_all('span', class_=\"o-Ingredients__a-Ingredient--CheckboxLabel\")\n",
    "        ingredients_list = ingredients_list[1:] # Remove the \"Deselect All\" from the list of ingredients\n",
    "        directions_elements = soup.find_all('li', class_='o-Method__m-Step')\n",
    "        directions = [direction.get_text(strip=True) for direction in directions_elements]\n",
    "        cook_note_element = soup.find('p', class_='o-ChefNotes__a-Description')\n",
    "        cook_note = cook_note_element.get_text(strip=True) if cook_note_element else 'Cook\\'s note not found'\n",
    "\n",
    "        # Extract the categories\n",
    "        category_elements = soup.find_all('a', class_='o-Capsule__a-Tag a-Tag')\n",
    "        categories = [category.get_text(strip=True) for category in category_elements]\n",
    "\n",
    "    recipe_info = {\n",
    "        'title': title,\n",
    "        'level': level,\n",
    "        'yield': servings,\n",
    "        'cooking time': cooking_time,\n",
    "        'ingredients': ingredients_list, ## TODO: before adding ingredients to recipe info\n",
    "        'steps': directions,\n",
    "        'cook note': cook_note,\n",
    "        'categories': categories\n",
    "    }\n",
    "    \n",
    "    all_ingredients.update(recipe_info['ingredients']) ## TODO: remove the measurments before adding to ingredients\n",
    "    all_categories.update(recipe_info['categories'])\n",
    "    \n",
    "    return recipe_info\n",
    "\n",
    "\n",
    "## Writes data to a JSON file. ##\n",
    "def write_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.foodnetwork.com/recipes/food-network-kitchen/123\"  # Starting with '123'\n",
    "    urls = generate_urls(base_url)\n",
    "    recipe_index = {}\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Processing {url}...\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract recipes from current page and scrape them\n",
    "        recipes = get_recipes_from_page(url)\n",
    "        print(f\"Found {len(recipes)} recipes.\")\n",
    "\n",
    "        # Iterate over each URL to access and extract information\n",
    "        for recipe in recipes:\n",
    "            recipe_id = str(uuid.uuid4())\n",
    "            details = scrape_recipe_details(recipe)\n",
    "            recipe_index[recipe_id] = details['title']\n",
    "            write_json(details, f\"{recipe_id}.json\")\n",
    "        print(f\"{url} has been processed.\")\n",
    "    \n",
    "    print(\"All URLs have been processed.\")\n",
    "    \n",
    "    # Writing the index file and ingredient/category accumulations\n",
    "    write_json(recipe_index, \"recipe_index.json\")\n",
    "    write_json(list(all_ingredients), \"all_ingredients.json\")\n",
    "    write_json(list(all_categories), \"all_categories.json\")\n",
    "\n",
    "# main() TODO: UNCOMMENT TO RUN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
