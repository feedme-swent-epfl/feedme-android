{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Global sets to store unique ingredients and categories\n",
    "all_ingredients = set()\n",
    "all_categories = set()\n",
    "\n",
    "\n",
    "# Dictionary mapping Unicode fraction symbols to their decimal equivalents\n",
    "fraction_mapping = {\n",
    "    '\\u00BD': '1/5',  # 1/2\n",
    "    '\\u00BC': '1/4', # 1/4\n",
    "    '\\u00BE': '3/4', # 3/4\n",
    "    '\\u2153': '1/3', # 1/3\n",
    "    '\\u2154': '2/3', # 2/3\n",
    "    '\\u215b': '1/8', # 1/8\n",
    "    '\\u00b0': 'degrees ', #Â°\n",
    "    '\\u2013': '-', # -\n",
    "    '\\u00a0': ' ' # space\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def replace_fraction_symbols(text):\n",
    "    # Define a regex pattern that captures numbers possibly adjacent to fraction symbols\n",
    "    pattern = r'(\\d*)(%s)(\\d*)' % '|'.join(re.escape(key) for key in fraction_mapping.keys())\n",
    "\n",
    "    # Function to replace each match with appropriate spacing\n",
    "    def replace(match):\n",
    "        # Pre-number, fraction, and post-number\n",
    "        pre, frac, post = match.groups()\n",
    "        # Replace the fraction with its decimal equivalent from the dictionary\n",
    "        frac_decimal = fraction_mapping[frac]\n",
    "        # Add space if there is a preceding or succeeding number\n",
    "        if frac_decimal == 'degrees ' or frac_decimal == '-' or frac_decimal == ' ':\n",
    "            if pre and post:\n",
    "                return f'{pre} {frac_decimal} {post}'\n",
    "            elif pre:\n",
    "                return f'{pre} {frac_decimal}'\n",
    "            elif post:\n",
    "                return f'{frac_decimal} {post}' \n",
    "        elif pre and post:\n",
    "            return f'{pre} and {frac_decimal} {post}'\n",
    "        elif pre:\n",
    "            return f'{pre} and {frac_decimal}'\n",
    "        elif post:\n",
    "            return f'{frac_decimal} {post}'\n",
    "        return frac_decimal\n",
    "\n",
    "    return re.sub(pattern, replace, text)\n",
    "\n",
    "\n",
    "## Extracts all recipe URLs from a given page. ##\n",
    "def get_recipes_from_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Define the base URL\n",
    "    base_url = \"https://en.wikibooks.org\"\n",
    "    \n",
    "    # Find all the 'a' tags with 'href' attributes in the soup object\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Initialize a list to store the complete URLs\n",
    "    full_urls = []\n",
    "\n",
    "    # Loop through all the 'a' tags\n",
    "    for link in links:\n",
    "        # Check if the 'href' attribute is a recipe link\n",
    "        if \"Category:Recipes\" in link['href']:\n",
    "            # Concatenate the base URL with the 'href' attribute and add it to the list\n",
    "            full_urls.append(base_url + link['href'])\n",
    "    \n",
    "    return full_urls\n",
    "\n",
    "\n",
    "def find_next_page_url(soup):\n",
    "    next_page_link = soup.select_one(\"a[title='Category:Recipes']\")\n",
    "    if next_page_link and \"next page\" in next_page_link.text:\n",
    "        return next_page_link['href']\n",
    "    return None\n",
    "\n",
    "\n",
    "def construct_full_url(relative_url):\n",
    "    base_url = \"https://en.wikibooks.org\"\n",
    "    return base_url + relative_url if relative_url else Non\n",
    "\n",
    "\n",
    "## Finds the URL for the next page based on alphabetical navigation. ##\n",
    "def generate_urls(base_url):\n",
    "    # Initialize a list to keep track of all next page URLs\n",
    "    next_page_urls = []\n",
    "\n",
    "    # Load the first page\n",
    "    html_content = load_html('/mnt/data/output.html')\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the first \"next page\" URL\n",
    "    next_page_relative_url = find_next_page_url(soup)\n",
    "    while next_page_relative_url:\n",
    "        # Construct the full URL and add to the list\n",
    "        full_url = construct_full_url(next_page_relative_url)\n",
    "        next_page_urls.append(full_url)\n",
    "        \n",
    "        # Load the next page content (this part is for illustration and would need actual page content to work)\n",
    "        # next_page_html_content = load_html(full_url)  # You would need to fetch the next page content here\n",
    "        # soup = BeautifulSoup(next_page_html_content, 'html.parser')\n",
    "        \n",
    "        # Find the next \"next page\" URL\n",
    "        next_page_relative_url = find_next_page_url(soup)\n",
    "    \n",
    "    return next_page_urls\n",
    "\n",
    "\n",
    "## Scrapes detailed information about a recipe from its page. ##\n",
    "def scrape_recipe_details(url):\n",
    "    time.sleep(random.uniform(5.0, 10.0))  # Random delay between requests\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    response = requests.get(url)\n",
    "            \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Attempting to extract data based on typical Wikibooks structure\n",
    "        full_title = soup.title.text.replace(\" - Wikibooks, open books for an open world\", \"\").strip()\n",
    "        # Use regex to find everything after \"Cookbook:\"\n",
    "        match = re.search(r'Cookbook:(.*)', full_title)\n",
    "        if match:\n",
    "            title = match.group(1).strip()\n",
    "        else:\n",
    "            title = full_title  # Fallback to the full title if \"Cookbook:\" is not found\n",
    "        difficulty = soup.find('table', class_='infobox')\n",
    "        if difficulty:\n",
    "            difficulty_row = difficulty.find('th', text=lambda text: text and 'Difficulty' in text)\n",
    "            if difficulty_row:\n",
    "                # The difficulty value might be represented by an image alt text\n",
    "                difficulty_image = difficulty_row.find_next_sibling('td').find('img')\n",
    "                if difficulty_image and 'alt' in difficulty_image.attrs:\n",
    "                    difficulty = difficulty_image['alt'].strip()\n",
    "                else:\n",
    "                    difficulty = \"Difficulty image or alt text not found.\"\n",
    "            else:\n",
    "                difficulty = \"Difficulty row not found.\"\n",
    "        else:\n",
    "            difficulty = \"Infobox table not found.\"\n",
    "\n",
    "        servings = soup.find('th', text='Servings').find_next_sibling('td').text if soup.find('th', text='Servings') else 'Not specified'\n",
    "\n",
    "        cooking_time = soup.find('table', class_='infobox')\n",
    "        if cooking_time:\n",
    "            time_row = cooking_time.find('th', string='Time')\n",
    "            if time_row:\n",
    "                time_data = time_row.find_next_sibling('td')\n",
    "                if time_data:\n",
    "                    time_text = time_data.text.strip()\n",
    "                    # Use regex to capture everything after \"Cooking:\"\n",
    "                    match = re.search(r\"Cooking:\\s*(.*)\", time_text)\n",
    "                    if match:\n",
    "                        cooking_time = match.group(1).strip()  # Give only the text after \"Cooking:\"\n",
    "                    else:\n",
    "                        cooking_time = time_text\n",
    "                else:\n",
    "                    cooking_time = \"Cooking time data not found.\"\n",
    "            else:\n",
    "                cooking_time = \"Time row not found.\"\n",
    "        else:\n",
    "            cooking_time = \"Infobox table not found.\"\n",
    "\n",
    "        ingredients_list = [li.text.strip() for li in soup.find('span', text='Ingredients').parent.find_next_sibling('ul').find_all('li')] if soup.find('span', text='Ingredients') else []\n",
    "        \n",
    "        directions_list = soup.find('ol')\n",
    "        if directions_list:\n",
    "            directions_list = [li.text.strip() for li in directions_list.find_all('li')]\n",
    "        else:\n",
    "            directions_list = [\"Procedure section not found.\"]\n",
    "        \n",
    "        categories_elements = [a.text for a in soup.find_all('a', href=True) if 'Category:' in a['href']]\n",
    "        \n",
    "        procedure_span = soup.find('span', string='Procedure')\n",
    "        notes_list = []\n",
    "        if procedure_span and procedure_span.parent:\n",
    "            notes_ul = procedure_span.parent.find_next_sibling('ul')\n",
    "            if notes_ul:\n",
    "                # Find all list item elements within the <ul> and extract their text\n",
    "                notes_list = [li.text.strip() for li in notes_ul.find_all('li')]\n",
    "            if not notes_list:  # If the list is empty, no notes were found\n",
    "                notes_list = ['No notes available']\n",
    "        else:\n",
    "            notes_list = ['No procedure section found.']\n",
    "\n",
    "    recipe_info = {\n",
    "        'title': title,\n",
    "        'level': difficulty,\n",
    "        'yield': servings,\n",
    "        'cooking time': replace_fraction_symbols(cooking_time),\n",
    "        'ingredients': [replace_fraction_symbols(string) for string in ingredients_list], ## TODO: before adding ingredients to recipe info\n",
    "        'steps': [replace_fraction_symbols(string) for string in directions_list],\n",
    "        'cook note': [replace_fraction_symbols(string) for string in notes_list],\n",
    "        'categories': categories_elements\n",
    "    }\n",
    "    \n",
    "    all_ingredients.update(recipe_info['ingredients']) ## TODO: remove the measurments before adding to ingredients\n",
    "    all_categories.update(recipe_info['categories'])\n",
    "    \n",
    "    return recipe_info\n",
    "\n",
    "\n",
    "## Writes data to a JSON file. ##\n",
    "def write_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "def main():\n",
    "    counter = 0\n",
    "    base_url = \"https://en.wikibooks.org/wiki/Category:Recipes\"\n",
    "    urls = generate_urls(base_url)\n",
    "    recipe_index = {}\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Processing {url}...\")\n",
    "\n",
    "        # Extract recipes from current page and scrape them\n",
    "        recipes = get_recipes_from_page(url)\n",
    "        print(f\"Found {len(recipes)} recipes.\")\n",
    "\n",
    "        # Iterate over each URL to access and extract information\n",
    "        for recipe in recipes:\n",
    "            details = scrape_recipe_details(recipe)\n",
    "            recipe_index[counter] = details\n",
    "            counter += 1\n",
    "            print(f\"{url} has been processed.\")\n",
    "        print(f\"{url} has been processed.\")\n",
    "    \n",
    "    print(\"All URLs have been processed.\")\n",
    "    \n",
    "    # Writing the index file and ingredient/category accumulations\n",
    "    write_json(recipe_index, \"recipe_index.json\")\n",
    "    write_json(list(all_ingredients), \"all_ingredients.json\")\n",
    "    write_json(list(all_categories), \"all_categories.json\")\n",
    "\n",
    "# main() TODO: UNCOMMENT TO RUN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
