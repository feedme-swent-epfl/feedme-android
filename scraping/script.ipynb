{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 3-Ingredient Bundt Cake\n",
      "Level: Easy\n",
      "Yield: 8 to 10 servings\n",
      "Cooking Time: 1 hr 40 min\n",
      "Ingredients:\n",
      "- Deselect All\n",
      "- Nonstick cooking spray, for the pan\n",
      "- One 15.25-ounce box cake mix (any flavor)\n",
      "- 1 pint high-quality ice cream (any flavor), completely melted\n",
      "- 3 large eggs\n",
      "Directions:\n",
      "Step 1: Preheat the oven to 350 degrees F. Thoroughly spray a 12-cup Bundt pan with the cooking spray, making sure to cover the entire inner surface.\n",
      "Step 2: Whisk together the cake mix, ice cream and eggs in a large bowl until well combined, then pour into the prepared Bundt pan. Bake until a cake tester inserted in middle of cake ring comes out clean and the sides of the cake are beginning to pull away from the edge of the pan, 35 to 40 minutes. Cool the cake in the pan for 20 minutes, then place a wire rack over the pan and invert the cake onto the rack. Cool completely.\n",
      "Cook's Note: Cook's note not found\n",
      "Categories: Bundt Cake, Dairy Recipes, Dessert\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Global sets to store unique ingredients and categories\n",
    "all_ingredients = set()\n",
    "all_categories = set()\n",
    "\n",
    "## Extracts all recipe URLs from a given page. ##\n",
    "def get_recipes_from_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    recipe_links = [a['href'] for a in soup.find_all('a', href=True) if \"/recipes/\" in a['href']]\n",
    "    return recipe_links\n",
    "\n",
    "\n",
    "## Finds the URL for the next page based on alphabetical navigation. ##\n",
    "def generate_urls(base_url):\n",
    "    parts = base_url.split(\"123\")\n",
    "    urls = [parts[0] + letter + parts[1] for letter in list(map(chr, range(97, 120)))]  # 'a' to 'w'\n",
    "    urls.append(parts[0] + \"xyz\" + parts[1])  # Adding 'xyz'\n",
    "    return urls\n",
    "\n",
    "\n",
    "## Scrapes detailed information about a recipe from its page. ##\n",
    "def scrape_recipe_details(url):\n",
    "    time.sleep(random.uniform(5.0, 10.0))  # Random delay between requests\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    response = requests.get(url)\n",
    "            \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract data using BeautifulSoup's methods\n",
    "        title = soup.find('span', class_=\"o-AssetTitle__a-HeadlineText\").get_text(strip=True)\n",
    "        level_headline = soup.find('span', class_='o-RecipeInfo__a-Headline')\n",
    "        level = level_headline.find_next_sibling('span').get_text(strip=True) if level_headline else 'Level not found'\n",
    "        yield_headline = soup.find('span', string='Yield:')\n",
    "        servings = yield_headline.find_next_sibling('span').get_text(strip=True) if yield_headline else 'Servings not found'\n",
    "        cooking_time = soup.find('span', class_=\"o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total\").get_text(strip=True)\n",
    "        ingredients_list = soup.find_all('span', class_=\"o-Ingredients__a-Ingredient--CheckboxLabel\")\n",
    "        directions_elements = soup.find_all('li', class_='o-Method__m-Step')\n",
    "        directions = [direction.get_text(strip=True) for direction in directions_elements]\n",
    "        cook_note_element = soup.find('p', class_='o-ChefNotes__a-Description')\n",
    "        cook_note = cook_note_element.get_text(strip=True) if cook_note_element else 'Cook\\'s note not found'\n",
    "\n",
    "        # Extract the categories\n",
    "        category_elements = soup.find_all('a', class_='o-Capsule__a-Tag a-Tag')\n",
    "        categories = [category.get_text(strip=True) for category in category_elements]\n",
    "\n",
    "    recipe_info = {\n",
    "        'title': title,\n",
    "        'level': level,\n",
    "        'yield': servings,\n",
    "        'cooking time': cooking_time,\n",
    "        'ingredients': ingredients_list, ## TODO: before adding ingredients to recipe info, remove the \"- Deselect All\"\n",
    "        'steps': directions,\n",
    "        'cook note': cook_note,\n",
    "        'categories': categories\n",
    "    }\n",
    "    \n",
    "    all_ingredients.update(recipe_info['ingredients']) ## TODO: remove the measurments before adding to ingredients\n",
    "    all_categories.update(recipe_info['categories'])\n",
    "    \n",
    "    return recipe_info\n",
    "\n",
    "\n",
    "## Writes data to a JSON file. ##\n",
    "def write_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.foodnetwork.com/recipes/food-network-kitchen/123\"  # Starting with '123'\n",
    "    urls = generate_urls(base_url)\n",
    "    recipe_index = {}\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Processing {url}...\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract recipes from current page and scrape them\n",
    "        recipes = get_recipes_from_page(url)\n",
    "        print(f\"Found {len(recipes)} recipes.\")\n",
    "\n",
    "        # Iterate over each URL to access and extract information\n",
    "        for recipe in recipes:\n",
    "            recipe_id = str(uuid.uuid4())\n",
    "            details = scrape_recipe_details(recipe)\n",
    "            recipe_index[recipe_id] = details['title']\n",
    "            write_json(details, f\"{recipe_id}.json\")\n",
    "        print(f\"{url} has been processed.\")\n",
    "    \n",
    "    print(\"All URLs have been processed.\")\n",
    "    \n",
    "    # Writing the index file and ingredient/category accumulations\n",
    "    write_json(recipe_index, \"recipe_index.json\")\n",
    "    write_json(list(all_ingredients), \"all_ingredients.json\")\n",
    "    write_json(list(all_categories), \"all_categories.json\")\n",
    "\n",
    "# main() TODO: UNCOMMENT TO RUN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
